---
title: "Technical details"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Technical details}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

<center>

![Figure 1: (a): The piecewise mean structure of $\beta$ according to a two-dimensional grid graph. (b): Under the null hypothesis, both $p_{\text{Hyun}}$ and $p_{C_1,C_2}$ control the selective Type I error, but the z-test $p_{\text{Naive}} = \mathbb{P}(|\nu^\top Y|\geq |\nu^\top y|)$ leads to inflated selective Type I error. (c): For a given value of the effect size ($|\nu^\top\beta|/\sigma$), $p_{C_1,C_2}$ has higher power than $p_{\text{Hyun}}$. Power for both increases as a function of the effect size.](../man/figures/combined_two_d.png){width=80%}

</center>


### Overview

The graph fused lasso (GFL) is widely used to reconstruct signals that are piecewise constant on a pre-defined graph, meaning that nodes connected by an edge in the graph tend to have identical values. Many special case of the graph fused lasso have been studied in the literature. For instance, when the underlying graph is a one-dimensional chain graph, it's known as the one-dimensional fused lasso, a well-studied method for changepoint detection. When the underlying graph is two- or three-dimensional grid graph, the GFL is known as the total-variational denoising. 

<!-- One desirable property of the GFL is that we can segment the reconstructed signals into *connected components*, that is, sets of elements of the reconstructed signals that share a common value and are connected in the original graph. -->

We consider testing for a difference in the means of two connected components estimated using the graph fused lasso. A naive procedure such as a z-test for a difference in means will not control the selective Type I error &mdash; the probability of falsely rejecting the null hypothesis *given that we decided to conduct the test* &mdash; since the hypothesis that we are testing is itself a function of the data. 

In our manuscript, we propose a new test for this task that controls the selective Type I error, and conditions on much less information than existing approaches, leading to substantially higher power. In addition, we provide a computationally-efficient implementation of the proposed p-value. 

### Model setup
We consider a vector $Y \in \mathbb{R}^p$, assumed to be a noisy realization of a 
signal $\beta \in \mathbb{R}^p$, 
$$Y_j = \beta_j + \epsilon_j , \quad \epsilon_j \sim_{\text{i.i.d.}} \mathcal{N}(0, \sigma^2), \quad j =1, \ldots, p,$$
 with known variance $\sigma^2$.  
 We assume that $\beta$ is *piecewise constant*, meaning that the elements of $\beta$ are ordered, and adjacent elements tend to take on equal values.
 
### Graph fused lasso 
It is natural to estimate $\beta$ by the graph fused lasso, 
$$
  \hat{\beta} = \text{argmin}_{\beta \in \mathbb{R}^p} \left\{ \frac{1}{2}\Vert y-\beta\Vert_2^2 +\lambda \sum_{(j,j')\in E}|\beta_j-\beta_{j'}| \right\},
$$
where $G=(V,E)$ is an undirected graph,  $V=\{1,\ldots,p\}$,  and  $(j,j') \in E$ indicates that the $j$th and $j'$th vertices in the graph are connected by an edge. 

For sufficiently large values of the non-negative tuning parameter $\lambda$, we will have $\hat\beta_j = \hat\beta_{j'}$ for some $(j,j') \in E$. We can segment $\hat\beta$ into *connected components* &mdash;  that is, sets of elements of $\hat\beta$ that are connected in the original graph and share a common value. 

### Inference for the difference in means of two connected components

We consider testing the null hypothesis that the true mean of $\beta$ is the same across two *estimated* connected components, i.e.,
$$  H_0: {\sum_{j \in \hat{C}_1} \beta_j  }/{|\hat{C}_1|  }  =
 { \sum_{j' \in \hat{C}_2} \beta_{j'} }/{   |\hat{C}_2|} \mbox{ versus }  H_1: {\sum_{j \in \hat{C}_1} \beta_j  }/{|\hat{C}_1|  }  \neq
 { \sum_{j' \in \hat{C}_2} \beta_{j'} }/{   |\hat{C}_2|},
$$
where $\hat{C}_1\subseteq V$ and $\hat{C}_2\subseteq V$ are two connected components of $\hat\beta$, with cardinality $|\hat{C}_1|$ and $|\hat{C}_2|$. This is equivalent to testing 
  $H_0: \nu^\top \beta = 0$ versus $H_1: \nu^\top \beta \neq 0$, where  $\nu \in \mathbb{R}^n$ is defined as 
$$  \nu_{i} =  1_{( i \in \hat{C}_1)} / |\hat{C}_1| -  1_{( i \in \hat{C}_2)}/|\hat{C}_2|.$$

This results in a challenging selective inference problem because we need to account for the process that led us to test this very hypothesis! Drawing from the  selective inference literature, Hyun et al. (2018) tackled this problem by proposing the following $p$-value: 
 $$p_{\text{Hyun}} = \mathbb{P}_{H_0}\left(|\nu^{\top} Y |\geq |\nu^{\top} y| \;\middle |\; \bigcap_{k=1}^K \left\{ M_k(Y) = M_k(y) \right\}, \Pi_{\nu}^{\perp} Y= \Pi_{\nu}^{\perp} y\right),$$
 where ${M}_k(y)$ is the output of the dual-path algorithm at k-th step (see Section 2 of Chen et al. (2021+) and Hyun et al. (2018) for (i) a detailed discussion of the dual-path algorithm to solve the GFLl and (ii) the definition of ${M}_k(y)$); and $\Pi_{\nu}^{\perp}$ is an orthogonal projection operator used to eliminate nuisance parameters. The key insight in Hyun et al. (2018)  is that $p_{\text{Hyun}}$ can be recast as the survival function of a normal distribution with mean 0, variance $\sigma^2||\nu||_2^2$, truncated to an interval that can be analytically computed from the data. 

Our paper relies on a simple observation: the $p$-value $p_{\text{Hyun}}$ conditions on too much information (i.e., it conditions on all of the outputs of the first $K$ steps of the dual path algorithm). However, typically the contrast vector $\nu$ in $H_0: \nu^{\top} \beta=0$ is constructed using only the presence of the two connected components of interest.

Therefore, we propose the following $p$-value
$$
p_{C_1,C_2} =  \mathbb{P}_{H_0}\left(|\nu^\top Y |\geq |\nu^\top y|  \;\middle\vert\;  \hat{C}_1(y),  \hat{C}_2(y) \in \mathcal{CC}_K(Y), \Pi_{\nu}^{\perp} Y= \Pi_{\nu}^{\perp} y \right),
$$
where  $\hat{C}_1,\hat{C}_2$ are two estimated connected components used to construct $\nu$, and $\mathcal{CC}_K(Y)$ is the set of *all* connected components after running $K$-step dual-path algorithm on data $Y$. We show that this $p$-value for testing $H_0$ can be written as 
$$
p_{C_1,C_2} = \mathbb{P}\left(|\phi|\geq |\nu^\top y| \;\middle\vert\; \hat{C}_1(y),\hat{C}_2(y) \in \mathcal{CC}_K(y'(\phi))\right),
$$
where $\phi\sim \mathcal{N}(0,\sigma^2||\nu||_2^2)$, and  $y'(\phi) = \Pi_{\nu}^{\perp}y+\phi\cdot \frac{\nu}{||\nu||_2^2} = y+\left(\frac{\phi-\nu^{\top}y}{||\nu||_2^2}\right)\nu$ is a perturbation of the observed data $y$, along the direction of $\nu$. 

Moreover, if we denote $\mathcal{S}_{C_1,C_2}=\{\phi: \hat{C}_1,\hat{C}_2 \in \mathcal{CC}_K(y'(\phi))\}$, the $p$-value can be equivalently expressed as
$\mathbb{P}\left(\phi \geq \nu^\top y \;\middle |\; \phi \in \mathcal{S}_{C_1,C_2} \right);$ thus the computation of the $p$-value amounts to characterizing the set $\mathcal{S}_{C_1,C_2}$.

Of course, computing $\mathcal{S}_{C_1,C_2}$ is a challenging task, because instead of a single union, it could be a potentially exponential number of union of intervals $\mathcal{S}_{C_1,C_2} = \bigcup_{i\in\mathcal{I}} (a_i,a_{i+1})$. In our work, we make use of a recent algorithm proposed in Jewell et al. (2019) to characterize the set $\mathcal{S}_{C_1,C_2}$ efficiently. The key insight of our procedure is that empirically, only a small subset of the exponential number of intervals is relevant to computing $\mathcal{S}_{C_1,C_2}$; moreover, we can identify the subset $\mathcal{I}$ with an efficient line search algorithm.

Our software implements an efficient calculation of $p_{C_1,C_2}$, and the resulting $p$-value will control the selective Type I error, while having higher power than $p_{\text{Hyun}}$. Additional details and extensions can be found in Sections 3 and 4 of our paper (Chen et al. 2021)).

### Extensions
Our software also allows for the following extensions:

1. Generate $(1-\alpha)$ selective confidence intervals for the parameter $\nu^\top \beta$, the difference in means of between connected components.
2. Condition on the output of c connected components (instead of K, number of steps of the dual-path algorithm).

### References

Chen YT, Jewell SW, Witten DM. (2021+) More powerful selective inference for the graph fused lasso

Fithian W, Sun D, Taylor J. (2014) Optimal Inference After Model Selection. arXiv:1410.2597 [mathST]. 

Hyun S, Gâ€™Sell M, Tibshirani RJ. (2018) Exact post-selection inference for the generalized lasso path. Electron J Stat.

Jewell S, Fearnhead P, Witten D. Testing for a Change in Mean After Changepoint Detection. arXiv [statME]. Published online October 9, 2019. http://arxiv.org/abs/1910.04291

Lee JD, Sun DL, Sun Y, Taylor JE. Exact post-selection inference, with application to the lasso. Ann Stat. 2016;44(3):907-927. doi:10.1214/15-AOS1371




